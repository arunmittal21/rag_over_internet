{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0cb89a9f-05c7-49c8-83f3-cc0a8a31d344",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-16T00:05:14.236930Z",
     "iopub.status.busy": "2025-06-16T00:05:14.236068Z",
     "iopub.status.idle": "2025-06-16T00:05:14.253904Z",
     "shell.execute_reply": "2025-06-16T00:05:14.252835Z",
     "shell.execute_reply.started": "2025-06-16T00:05:14.236898Z"
    }
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'langgraph.prebuilt.tool_executor'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 6\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mlangchain_core\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mprompts\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m ChatPromptTemplate\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mlangchain\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtools\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m tool\n\u001b[0;32m----> 6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mlanggraph\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mprebuilt\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtool_executor\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m ToolExecutor, ToolInvocation\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mlangchain_core\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mrunnables\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m RunnableLambda\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mjson\u001b[39;00m\u001b[38;5;241m,\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mre\u001b[39;00m \n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'langgraph.prebuilt.tool_executor'"
     ]
    }
   ],
   "source": [
    "\n",
    "from langgraph.graph import StateGraph, END\n",
    "# from langchain_openai import ChatOpenAI\n",
    "from langchain_community.chat_models import BedrockChat\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain.tools import tool\n",
    "from langgraph.prebuilt.tool_executor import ToolExecutor, ToolInvocation\n",
    "from langchain_core.runnables import RunnableLambda\n",
    "import json, re \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f5b251f2-fa12-4a7b-9059-5327dcf699c2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-16T00:01:48.128278Z",
     "iopub.status.busy": "2025-06-16T00:01:48.127990Z",
     "iopub.status.idle": "2025-06-16T00:01:49.173486Z",
     "shell.execute_reply": "2025-06-16T00:01:49.172104Z",
     "shell.execute_reply.started": "2025-06-16T00:01:48.128253Z"
    }
   },
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'ToolExecutor' from 'langgraph.prebuilt' (/opt/conda/lib/python3.12/site-packages/langgraph/prebuilt/__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 8\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mlangchain_core\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mprompts\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m ChatPromptTemplate\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mlangchain\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtools\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m tool\n\u001b[0;32m----> 8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mlanggraph\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mprebuilt\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m ToolExecutor, ToolInvocation\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mlangchain_core\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mrunnables\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m RunnableLambda\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mjson\u001b[39;00m\u001b[38;5;241m,\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mre\u001b[39;00m \n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'ToolExecutor' from 'langgraph.prebuilt' (/opt/conda/lib/python3.12/site-packages/langgraph/prebuilt/__init__.py)"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# ---------------- TOOLS ----------------\n",
    "@tool\n",
    "def get_capital(country: str) -> str:\n",
    "    capitals = {\"France\": \"Paris\", \"Germany\": \"Berlin\"}\n",
    "    return capitals.get(country, \"Unknown\")\n",
    "\n",
    "@tool\n",
    "def get_population(country: str) -> str:\n",
    "    pops = {\"France\": 68000000, \"Germany\": 83000000}\n",
    "    return str(pops.get(country, 0))\n",
    "\n",
    "@tool\n",
    "def calculate_difference(values: str) -> str:\n",
    "    a, b = map(int, values.split(\",\"))\n",
    "    return str(abs(a - b))\n",
    "\n",
    "tools = [get_capital, get_population, calculate_difference]\n",
    "tool_executor = ToolExecutor(tools)\n",
    "\n",
    "# ---------------- PLANNER ----------------\n",
    "# planner_llm = ChatOpenAI(model=\"gpt-4\", temperature=0)\n",
    "planner_llm = BedrockChat(\n",
    "    model_id=\"anthropic.claude-3-sonnet-20240229-v1:0\",  # or another model like \"ai21.j2-ultra\" etc.\n",
    "    region_name=\"us-east-1\",  # or your region\n",
    "    temperature=0,\n",
    "    max_tokens=2048\n",
    ")\n",
    "\n",
    "def planner_agent(state):\n",
    "    query = state[\"query\"]\n",
    "    prompt = f\"\"\"\n",
    "You are a planner. Break down the query into high-level subtasks with executor type.\n",
    "Query: {query}\n",
    "Respond in JSON list like:\n",
    "[\n",
    "  {{\"executor\": \"researcher\", \"task\": \"Find Tesla's recent performance.\"}},\n",
    "  {{\"executor\": \"cot\", \"task\": \"Get capital of France.\"}},\n",
    "  {{\"executor\": \"math\", \"task\": \"Calculate revenue change from Q1 to Q2.\"}}\n",
    "]\n",
    "\"\"\"\n",
    "    response = planner_llm.invoke(prompt)\n",
    "    subtasks = json.loads(response.content)\n",
    "    return {\"tasks\": subtasks, \"current_step\": 0, \"results\": [], \"query\": query}\n",
    "\n",
    "# ---------------- EXECUTORS ----------------\n",
    "# llm = ChatOpenAI(model=\"gpt-4\", temperature=0)\n",
    "llm = BedrockChat(\n",
    "    model_id=\"anthropic.claude-3-sonnet-20240229-v1:0\",  # or another model like \"ai21.j2-ultra\" etc.\n",
    "    region_name=\"us-east-1\",  # or your region\n",
    "    temperature=0,\n",
    "    max_tokens=2048\n",
    ")\n",
    "\n",
    "REACT_PROMPT = \"\"\"\n",
    "You are a chain-of-thought executor with tools.\n",
    "Task: {task}\n",
    "Available tools: get_capital, get_population, calculate_difference\n",
    "Use this format:\n",
    "Thought: ...\n",
    "Action: ...\n",
    "Action Input: ...\n",
    "Observation: ...\n",
    "Final Answer: ...\n",
    "\"\"\"\n",
    "\n",
    "def cot_executor(state):\n",
    "    task = state[\"tasks\"][state[\"current_step\"]][\"task\"]\n",
    "    scratchpad = \"\"\n",
    "    for _ in range(5):\n",
    "        prompt = REACT_PROMPT.format(task=task) + scratchpad\n",
    "        output = llm.invoke(prompt).content.strip()\n",
    "        if \"Final Answer:\" in output:\n",
    "            answer = output.split(\"Final Answer:\")[1].strip()\n",
    "            return {**state, \"current_step\": state[\"current_step\"] + 1, \"results\": state[\"results\"] + [answer]}\n",
    "        m = re.search(r\"Action: (\\w+)\\s*Action Input: (.+)\", output)\n",
    "        if m:\n",
    "            tool, tool_input = m.group(1).strip(), m.group(2).strip()\n",
    "            result = tool_executor.invoke([ToolInvocation(tool, tool_input)])[0]\n",
    "            scratchpad += f\"{output}\\nObservation: {result}\\n\"\n",
    "    return state\n",
    "\n",
    "RESEARCH_PROMPT = \"\"\"\n",
    "You are a researcher. You should look up or synthesize knowledge from known facts.\n",
    "Task: {task}\n",
    "Respond with a summary of your findings.\n",
    "\"\"\"\n",
    "\n",
    "def researcher_executor(state):\n",
    "    task = state[\"tasks\"][state[\"current_step\"]][\"task\"]\n",
    "    response = llm.invoke(RESEARCH_PROMPT.format(task=task)).content.strip()\n",
    "    return {**state, \"current_step\": state[\"current_step\"] + 1, \"results\": state[\"results\"] + [response]}\n",
    "\n",
    "MATH_PROMPT = \"\"\"\n",
    "You are a math-focused executor. Solve the task directly.\n",
    "Task: {task}\n",
    "\"\"\"\n",
    "\n",
    "def math_executor(state):\n",
    "    task = state[\"tasks\"][state[\"current_step\"]][\"task\"]\n",
    "    response = llm.invoke(MATH_PROMPT.format(task=task)).content.strip()\n",
    "    return {**state, \"current_step\": state[\"current_step\"] + 1, \"results\": state[\"results\"] + [response]}\n",
    "\n",
    "# ---------------- ROUTER ----------------\n",
    "def route(state):\n",
    "    executor_type = state[\"tasks\"][state[\"current_step\"]][\"executor\"]\n",
    "    return executor_type\n",
    "\n",
    "# ---------------- GRAPH ----------------\n",
    "graph = StateGraph()\n",
    "\n",
    "graph.add_node(\"plan\", planner_agent)\n",
    "graph.add_node(\"researcher\", researcher_executor)\n",
    "graph.add_node(\"cot\", cot_executor)\n",
    "graph.add_node(\"math\", math_executor)\n",
    "\n",
    "graph.set_entry_point(\"plan\")\n",
    "graph.add_edge(\"plan\", \"router\")\n",
    "\n",
    "graph.add_conditional_edges(\"router\", route, {\n",
    "    \"researcher\": \"researcher\",\n",
    "    \"cot\": \"cot\",\n",
    "    \"math\": \"math\"\n",
    "})\n",
    "\n",
    "def is_done(state):\n",
    "    return state[\"current_step\"] >= len(state[\"tasks\"])\n",
    "\n",
    "graph.add_conditional_edges(\"researcher\", is_done, {True: END, False: \"router\"})\n",
    "graph.add_conditional_edges(\"cot\", is_done, {True: END, False: \"router\"})\n",
    "graph.add_conditional_edges(\"math\", is_done, {True: END, False: \"router\"})\n",
    "\n",
    "graph.set_finish_point(END)\n",
    "app = graph.compile()\n",
    "\n",
    "# ---------------- RUN ----------------\n",
    "query = \"Find Tesla's last quarter performance, get capital of France, and calculate revenue change from 10B to 12B.\"\n",
    "result = app.invoke({\"query\": query})\n",
    "print(\"\\nMulti-Agent Routed Execution:\")\n",
    "for r in result[\"results\"]:\n",
    "    print(\" -\", r)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
